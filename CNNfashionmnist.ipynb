{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set device\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "else: \n",
    "  device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.FashionMNIST(root = \"./data\",\n",
    "                                  train=True,\n",
    "                                  transform=transforms.ToTensor(),\n",
    "                                  download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root = \"./data\",\n",
    "                                  train=False,\n",
    "                                  transform=transforms.ToTensor(),\n",
    "                                  download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal',\n",
    "               'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set learning rate, batch size, and epochs\n",
    "learningRate = 0.001\n",
    "batchSize = 100\n",
    "numEpochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batchSize, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batchSize, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_image(arr):\n",
    "    two_d = (np.reshape(arr, (28, 28)) * 255).astype(np.uint8)\n",
    "    plt.imshow(two_d, interpolation='nearest', cmap='gray')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen_image(train_dataset[5][0].numpy()[0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define conv net class\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, numCategories=10): \n",
    "        super(ConvNet, self).__init__() #inherit info from parent class\n",
    "        #first sequential hidden layer\n",
    "        #combines convolution, ReLU and max pooling\n",
    "        self.layer1 = nn.Sequential(\n",
    "              nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),\n",
    "              nn.BatchNorm2d(num_features=16),\n",
    "              nn.ReLU(),\n",
    "              nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        #second sequential hidden layer\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        #output layer\n",
    "        self.fc = nn.Linear(7*7*32, numCategories)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x);\n",
    "        out = self.layer2(out);\n",
    "        out = out.reshape(out.size(0), -1);\n",
    "        out = self.fc(out);\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5]; batchStep = [100/600]; Loss: 0.4385.\n",
      "Epoch: [1/5]; batchStep = [200/600]; Loss: 0.4063.\n",
      "Epoch: [1/5]; batchStep = [300/600]; Loss: 0.3637.\n",
      "Epoch: [1/5]; batchStep = [400/600]; Loss: 0.2541.\n",
      "Epoch: [1/5]; batchStep = [500/600]; Loss: 0.2178.\n",
      "Epoch: [1/5]; batchStep = [600/600]; Loss: 0.2933.\n",
      "Epoch: [2/5]; batchStep = [100/600]; Loss: 0.3587.\n",
      "Epoch: [2/5]; batchStep = [200/600]; Loss: 0.1981.\n",
      "Epoch: [2/5]; batchStep = [300/600]; Loss: 0.1292.\n",
      "Epoch: [2/5]; batchStep = [400/600]; Loss: 0.3612.\n",
      "Epoch: [2/5]; batchStep = [500/600]; Loss: 0.2475.\n",
      "Epoch: [2/5]; batchStep = [600/600]; Loss: 0.2659.\n",
      "Epoch: [3/5]; batchStep = [100/600]; Loss: 0.1726.\n",
      "Epoch: [3/5]; batchStep = [200/600]; Loss: 0.2221.\n",
      "Epoch: [3/5]; batchStep = [300/600]; Loss: 0.3028.\n",
      "Epoch: [3/5]; batchStep = [400/600]; Loss: 0.1759.\n",
      "Epoch: [3/5]; batchStep = [500/600]; Loss: 0.2194.\n",
      "Epoch: [3/5]; batchStep = [600/600]; Loss: 0.2452.\n",
      "Epoch: [4/5]; batchStep = [100/600]; Loss: 0.1937.\n",
      "Epoch: [4/5]; batchStep = [200/600]; Loss: 0.2223.\n",
      "Epoch: [4/5]; batchStep = [300/600]; Loss: 0.1881.\n",
      "Epoch: [4/5]; batchStep = [400/600]; Loss: 0.1805.\n",
      "Epoch: [4/5]; batchStep = [500/600]; Loss: 0.3302.\n",
      "Epoch: [4/5]; batchStep = [600/600]; Loss: 0.1599.\n",
      "Epoch: [5/5]; batchStep = [100/600]; Loss: 0.1932.\n",
      "Epoch: [5/5]; batchStep = [200/600]; Loss: 0.1509.\n",
      "Epoch: [5/5]; batchStep = [300/600]; Loss: 0.3240.\n",
      "Epoch: [5/5]; batchStep = [400/600]; Loss: 0.2125.\n",
      "Epoch: [5/5]; batchStep = [500/600]; Loss: 0.3259.\n",
      "Epoch: [5/5]; batchStep = [600/600]; Loss: 0.2147.\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "numSteps = len(train_loader)\n",
    "for epoch in range(numEpochs):\n",
    "    for i, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        #forward\n",
    "        outputs = model(image)\n",
    "        loss = loss_func(outputs, label)\n",
    "        \n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if ((i + 1) % 100) == 0:\n",
    "          print ('Epoch: [%d/%d]; batchStep = [%d/%d]; Loss: %.4f.'%(epoch+1, numEpochs, i + 1, numSteps, loss.item()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.82\n"
     ]
    }
   ],
   "source": [
    "#test model\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += 1\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(correct/total)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
